{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpAUjnffjB6U"
      },
      "source": [
        "### Importing needed libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C4LGIZw-iu4H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4PSkr0DjXxQ"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CJxatSHPqZBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05e8b949-2f02-4ba4-a7d5-017d1aeea813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-01 14:40:37--  https://www.dropbox.com/scl/fi/kkrfmiskhap3rxvh6vv8b/pb3_train.txt?rlkey=13xaop0af6c7xhhwtpzfimuqc&dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.80.18, 2620:100:6020:18::a27d:4012\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.80.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca9c5a8ac5affb71066bf375384.dl.dropboxusercontent.com/cd/0/inline/COSo4c2W3w5XfVSebkWLQIFxo5IfuN7jL-BYdIDCyfpA4COIrxtKcH525FqbYzICk40YOoUDw1-sSt5ZvgCJCjyDPZXmiKB8Ral0h6hLiW7i5wPnikXypwBw16Gm3Xgr7FQuzYITKzqRl2ZQfuC1fzsw/file# [following]\n",
            "--2024-03-01 14:40:38--  https://uca9c5a8ac5affb71066bf375384.dl.dropboxusercontent.com/cd/0/inline/COSo4c2W3w5XfVSebkWLQIFxo5IfuN7jL-BYdIDCyfpA4COIrxtKcH525FqbYzICk40YOoUDw1-sSt5ZvgCJCjyDPZXmiKB8Ral0h6hLiW7i5wPnikXypwBw16Gm3Xgr7FQuzYITKzqRl2ZQfuC1fzsw/file\n",
            "Resolving uca9c5a8ac5affb71066bf375384.dl.dropboxusercontent.com (uca9c5a8ac5affb71066bf375384.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uca9c5a8ac5affb71066bf375384.dl.dropboxusercontent.com (uca9c5a8ac5affb71066bf375384.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 928181 (906K) [text/plain]\n",
            "Saving to: ‘./data_train.txt’\n",
            "\n",
            "./data_train.txt    100%[===================>] 906.43K  1.10MB/s    in 0.8s    \n",
            "\n",
            "2024-03-01 14:40:40 (1.10 MB/s) - ‘./data_train.txt’ saved [928181/928181]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://www.dropbox.com/scl/fi/kkrfmiskhap3rxvh6vv8b/pb3_train.txt?rlkey=13xaop0af6c7xhhwtpzfimuqc&dl=0' -O ./data_train.txt\n",
        "\n",
        "with open('data_train.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iejbATFsmb79"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wAxhlLwammKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64612fd1-a638-4150-91ac-90faa85d8997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-01 14:40:40--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  6.08MB/s    in 0.2s    \n",
            "\n",
            "2024-03-01 14:40:40 (6.08 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n",
            "Vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "class Tokenizer:\n",
        "    ''' Tokenizer class '''\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        ''' Initialize tokenizer '''\n",
        "\n",
        "        self.vocab = vocab\n",
        "        self.vocab_size = len(vocab)\n",
        "        self.stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "        self.encode = lambda s: [self.stoi[c] for c in s]\n",
        "        self.decode = lambda l: ''.join([self.itos[i] for i in l])\n",
        "\n",
        "# Load vocabulary\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    raw = f.read()\n",
        "vocab = sorted(set(raw))\n",
        "\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer = Tokenizer(vocab)\n",
        "print('Vocab size:', tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiehR5uvvaNy"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "08VQ0zV3vY-X"
      },
      "outputs": [],
      "source": [
        "# TODO: choose hyper-parameters\n",
        "batch_size = 32 # How many samples to process at each iteration (DO NOT CHANGE)\n",
        "block_size = 128  # Maximum context length (DO NOT CHANGE)\n",
        "max_iters = 20000 # Maximum number of iterations - change later -\n",
        "eval_interval = 100 # How often to evaluate the model\n",
        "learning_rate =  0.003 # Learning rate - change later -  1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Device (accelerator)\n",
        "eval_iters = 200 # Number of steps to evaluate the model\n",
        "n_embd = 64 # Embedding dimension (should be divisible by n_head) - change later\n",
        "n_head = 4 # Number of heads - change later\n",
        "n_layer = 4 # Number of layers - change later\n",
        "dropout = 0.01 # Dropout probability [0, 1] - change later"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGehScdnD5D7",
        "outputId": "e53a1275-6c87-4b8a-e4a4-685b746c2f97"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sdGQmU4uydY"
      },
      "source": [
        "### Train and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pzKoKuWIowk8"
      },
      "outputs": [],
      "source": [
        "''' Data tokenization and splits '''\n",
        "# TODO: complete the code to encode the text by the tokenizer\n",
        "tokenized_text = tokenizer.encode(text)\n",
        "data = torch.tensor(tokenized_text, dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33izXrcFvSCV"
      },
      "source": [
        "### Get batch of data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3YN9Qq9svRBG"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    ''' Generate a small batch of data of inputs x and targets y '''\n",
        "\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    # TODO: complete the code to implement next token prediction by defining y\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF288Akwv2ZC"
      },
      "source": [
        "### Estimate loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oYt_xPgavFrC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    ''' Estimate loss '''\n",
        "\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXY8eTt1wJph"
      },
      "source": [
        "### Transformer Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Mjk1N-w8wLjK"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # TODO: Complete the code by defining the key, query and value as Linear operations without bias\n",
        "        # Hint: dimensions of the linear operations must be n_embd x head_size\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # TODO: Transform the input x by apply the key and query operations to get k and q\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        # TODO: Implement the scaled self-attention mechanism by multiplying keys and queries\n",
        "        # Hint: scale by multiplying by C ** -0.5 and use .transpose(-2, -1).\n",
        "        A = q @ k.transpose(-2,-1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "\n",
        "        A = A.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        # TODO: Apply softmax function to A by setting dim=-1\n",
        "        # Hint: use the function F.softmax\n",
        "        A = F.softmax(A, dim=-1)\n",
        "\n",
        "\n",
        "        A = self.dropout(A)\n",
        "        # TODO: transform the input x to get the values\n",
        "        v = self.value(x) # b, t, c\n",
        "        # TODO: Multiply values by A to obtain the output\n",
        "        # Hint: the output should have the same shape as input x i.e. (B, T, C)\n",
        "        out = A @ v   # (b, t, t).(b, t, c)\n",
        "\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        ''' n_embd: embedding dimension, n_head: the number of heads '''\n",
        "\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass by applying the following operations:\n",
        "        # 1) apply layer norm ln1 to x\n",
        "        # 2) apply self-attention sa to the result in step 1)\n",
        "        # 3) add skip connection by adding the input x to the result in step 2)\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        # TODO: Implement the forward pass by applying the following operations:\n",
        "        # a) apply layer norm ln2 to the result in step 3)\n",
        "        # b) apply feed-forward ffwd to the result in step a)\n",
        "        # c) add skip connection by adding the input x to the result in step b)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9zUE6CxJYw"
      },
      "source": [
        "### Transformer Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xZQkZQzTwBKU"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    ''' A Transformer model '''\n",
        "\n",
        "    def __init__(self, n_layer):\n",
        "        ''' n_layer: the number of Transformer blocks '''\n",
        "\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(tokenizer.vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, tokenizer.vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # TODO: define tok_emb by applying token_embedding_table to the input idx\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        # TODO: add tok_emb and pos_emb to get x\n",
        "        x = tok_emb + pos_emb\n",
        "        # TODO: apply the blocks to get the output\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            # TODO: compute the cross entropy loss\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DL4KNNjxkVS"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XDwQF-G4xfVk"
      },
      "outputs": [],
      "source": [
        "model = Transformer(n_layer=n_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe7byAtOxroO"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPjiGAXoxpTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4727c847-6353-48ac-8ef1-0e067172f853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "215873 M parameters\n",
            "step 0: train loss 4.3157, val loss 4.3139\n",
            "step 100: train loss 2.6260, val loss 2.6627\n",
            "step 200: train loss 2.4993, val loss 2.5401\n",
            "step 300: train loss 2.4413, val loss 2.4867\n",
            "step 400: train loss 2.3846, val loss 2.4292\n",
            "step 500: train loss 2.3178, val loss 2.3771\n",
            "step 600: train loss 2.2508, val loss 2.3139\n",
            "step 700: train loss 2.2043, val loss 2.2676\n",
            "step 800: train loss 2.1481, val loss 2.2175\n",
            "step 900: train loss 2.0910, val loss 2.1677\n",
            "step 1000: train loss 2.0379, val loss 2.1237\n",
            "step 1100: train loss 1.9888, val loss 2.0834\n",
            "step 1200: train loss 1.9481, val loss 2.0525\n",
            "step 1300: train loss 1.9086, val loss 2.0200\n",
            "step 1400: train loss 1.8765, val loss 1.9930\n",
            "step 1500: train loss 1.8390, val loss 1.9704\n",
            "step 1600: train loss 1.8199, val loss 1.9561\n",
            "step 1700: train loss 1.7907, val loss 1.9419\n",
            "step 1800: train loss 1.7665, val loss 1.9239\n",
            "step 1900: train loss 1.7498, val loss 1.9170\n",
            "step 2000: train loss 1.7260, val loss 1.9050\n",
            "step 2100: train loss 1.7134, val loss 1.8972\n",
            "step 2200: train loss 1.6968, val loss 1.8903\n",
            "step 2300: train loss 1.6792, val loss 1.8707\n",
            "step 2400: train loss 1.6743, val loss 1.8729\n",
            "step 2500: train loss 1.6547, val loss 1.8635\n",
            "step 2600: train loss 1.6467, val loss 1.8537\n",
            "step 2700: train loss 1.6344, val loss 1.8479\n",
            "step 2800: train loss 1.6260, val loss 1.8444\n",
            "step 2900: train loss 1.6210, val loss 1.8471\n",
            "step 3000: train loss 1.6083, val loss 1.8314\n",
            "step 3100: train loss 1.5954, val loss 1.8352\n",
            "step 3200: train loss 1.5937, val loss 1.8339\n",
            "step 3300: train loss 1.5919, val loss 1.8316\n",
            "step 3400: train loss 1.5811, val loss 1.8359\n",
            "step 3500: train loss 1.5645, val loss 1.8207\n",
            "step 3600: train loss 1.5672, val loss 1.8235\n",
            "step 3700: train loss 1.5619, val loss 1.8262\n",
            "step 3800: train loss 1.5557, val loss 1.8274\n",
            "step 3900: train loss 1.5490, val loss 1.8115\n",
            "step 4000: train loss 1.5453, val loss 1.8083\n",
            "step 4100: train loss 1.5422, val loss 1.8168\n",
            "step 4200: train loss 1.5342, val loss 1.8219\n",
            "step 4300: train loss 1.5334, val loss 1.8119\n",
            "step 4400: train loss 1.5237, val loss 1.8107\n",
            "step 4500: train loss 1.5207, val loss 1.8066\n",
            "step 4600: train loss 1.5139, val loss 1.8092\n",
            "step 4700: train loss 1.5197, val loss 1.8010\n",
            "step 4800: train loss 1.5128, val loss 1.8000\n",
            "step 4900: train loss 1.5088, val loss 1.8020\n",
            "step 5000: train loss 1.5014, val loss 1.7982\n",
            "step 5100: train loss 1.5033, val loss 1.7958\n",
            "step 5200: train loss 1.5005, val loss 1.8035\n",
            "step 5300: train loss 1.4956, val loss 1.7849\n",
            "step 5400: train loss 1.4898, val loss 1.8018\n",
            "step 5500: train loss 1.4900, val loss 1.7853\n",
            "step 5600: train loss 1.4890, val loss 1.7821\n",
            "step 5700: train loss 1.4832, val loss 1.7908\n",
            "step 5800: train loss 1.4795, val loss 1.7965\n",
            "step 5900: train loss 1.4768, val loss 1.7816\n",
            "step 6000: train loss 1.4721, val loss 1.7736\n",
            "step 6100: train loss 1.4707, val loss 1.7644\n",
            "step 6200: train loss 1.4678, val loss 1.7710\n",
            "step 6300: train loss 1.4691, val loss 1.7890\n",
            "step 6400: train loss 1.4644, val loss 1.7797\n",
            "step 6500: train loss 1.4565, val loss 1.7597\n",
            "step 6600: train loss 1.4642, val loss 1.7802\n",
            "step 6700: train loss 1.4586, val loss 1.7770\n",
            "step 6800: train loss 1.4514, val loss 1.7656\n",
            "step 6900: train loss 1.4539, val loss 1.7733\n",
            "step 7000: train loss 1.4496, val loss 1.7617\n",
            "step 7100: train loss 1.4500, val loss 1.7715\n",
            "step 7200: train loss 1.4502, val loss 1.7649\n",
            "step 7300: train loss 1.4458, val loss 1.7648\n",
            "step 7400: train loss 1.4443, val loss 1.7633\n",
            "step 7500: train loss 1.4439, val loss 1.7604\n",
            "step 7600: train loss 1.4397, val loss 1.7704\n",
            "step 7700: train loss 1.4421, val loss 1.7625\n",
            "step 7800: train loss 1.4397, val loss 1.7619\n",
            "step 7900: train loss 1.4368, val loss 1.7548\n",
            "step 8000: train loss 1.4379, val loss 1.7676\n",
            "step 8100: train loss 1.4326, val loss 1.7513\n",
            "step 8200: train loss 1.4261, val loss 1.7546\n",
            "step 8300: train loss 1.4325, val loss 1.7653\n",
            "step 8400: train loss 1.4318, val loss 1.7533\n",
            "step 8500: train loss 1.4209, val loss 1.7511\n",
            "step 8600: train loss 1.4247, val loss 1.7581\n",
            "step 8700: train loss 1.4234, val loss 1.7531\n",
            "step 8800: train loss 1.4243, val loss 1.7541\n",
            "step 8900: train loss 1.4233, val loss 1.7604\n",
            "step 9000: train loss 1.4144, val loss 1.7510\n",
            "step 9100: train loss 1.4169, val loss 1.7512\n",
            "step 9200: train loss 1.4180, val loss 1.7500\n",
            "step 9300: train loss 1.4116, val loss 1.7617\n",
            "step 9400: train loss 1.4171, val loss 1.7385\n",
            "step 9500: train loss 1.4084, val loss 1.7468\n",
            "step 9600: train loss 1.4118, val loss 1.7522\n",
            "step 9700: train loss 1.4069, val loss 1.7629\n",
            "step 9800: train loss 1.4122, val loss 1.7490\n",
            "step 9900: train loss 1.4046, val loss 1.7344\n"
          ]
        }
      ],
      "source": [
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters()), 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "val_loss = []\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        val_loss.append(losses['val'])\n",
        "\n",
        "    # TODO: get a batch of training samples by calling the function get_batch\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # TODO: backpropagate on the loss\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYtRVdAMzMLK"
      },
      "outputs": [],
      "source": [
        "# plot validation loss\n",
        "plt.plot(val_loss)\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('val loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final model checkpoint and architecture\n",
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "230AVF18J5-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}